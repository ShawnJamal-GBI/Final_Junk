{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "from ftfy import fix_text\n",
    "# from util import UnitConversion, mapping_list_values, perl_to_posix\n",
    "from groupby_toolz import enrich_db, gcloud\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 500\n",
    "from flashtext import KeywordProcessor\n",
    "from groupby_toolz import enrich_db, gcloud\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import ast\n",
    "import warnings\n",
    "import regex as re\n",
    "warnings.filterwarnings('ignore')\n",
    "from decimal import *\n",
    "TWOPLACES = Decimal(10) ** -2\n",
    "from natsort import natsorted\n",
    "import ast\n",
    "import time\n",
    "today = time.strftime(\"%Y_%m_%d\")\n",
    "from enrich_dimensions.rounds import rounds, rounding_inch_feet,rounding_lbs,rounding_w,rounding_oz, rounding_lb,rounding_gal, re_extract, curate, round_string_float, clean_data,reg_ex,rounding_period_after_unit \n",
    "from enrich_dimensions.params import parameters, query_from_file\n",
    "from enrich_dimensions.query_file import query_from_file \n",
    "from enrich_dimensions.custom import custom_field \n",
    "\n",
    "\n",
    "def get_unique_list(material_list):\n",
    "    unique_list = []\n",
    "    for item in material_list:\n",
    "        if isinstance(item, str) and \"[\" in item and \"]\" in item:\n",
    "            item_values = ast.literal_eval(item)\n",
    "            unique_list.extend(item_values)\n",
    "        else:\n",
    "            unique_list.append(item)\n",
    "    return sorted(list(set(unique_list)))\n",
    "\n",
    "\n",
    "\n",
    "customer_id = '39'\n",
    "customer_name='%josabank%'\n",
    "client='Jos A Bank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Query of Curated Data\n",
      "19443\n"
     ]
    }
   ],
   "source": [
    "dateszs='2001-08-11'\n",
    "attribut='features'\n",
    "dateszs='2001-11-01'\n",
    "curation_col = f'Q:{attribut}'\n",
    "params = {'customer_id': customer_id,\n",
    "          'customer_name':customer_name,\n",
    "         'dateszs':dateszs,\n",
    "         'attribut':attribut}\n",
    "\n",
    "print('Start Query of Curated Data')\n",
    "cb = query_from_file(file_name='query/curated_all_attributes_date_family.sql', params=params)\n",
    "print(len(cb))\n",
    "\n",
    "fill_dict = cb.groupby('external_id')['buckets'].last().to_dict()\n",
    "cb['buckets'] = cb['buckets'].fillna(cb['external_id'].map(fill_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Attribute bucket Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "Number of attributes: 33\n",
      "\n",
      "Start Buckets\n",
      "0 Attribute: belt_width\n",
      "Number of SKUs for the attribute belt_width: 30\n",
      "Start Buckets\n",
      "1 Attribute: care\n",
      "Number of SKUs for the attribute care: 1663\n",
      "Start Buckets\n",
      "2 Attribute: collar_style\n",
      "Number of SKUs for the attribute collar_style: 583\n",
      "Start Buckets\n",
      "3 Attribute: color\n",
      "Number of SKUs for the attribute color: 184\n",
      "Start Buckets\n",
      "4 Attribute: cuff_style\n",
      "Number of SKUs for the attribute cuff_style: 491\n",
      "Start Buckets\n",
      "5 Attribute: cufflink_shape\n",
      "Number of SKUs for the attribute cufflink_shape: 15\n",
      "Start Buckets\n",
      "6 Attribute: features\n",
      "Number of SKUs for the attribute features: 1390\n",
      "Start Buckets\n",
      "7 Attribute: fit\n",
      "Number of SKUs for the attribute fit: 1270\n",
      "Start Buckets\n",
      "8 Attribute: hem_type\n",
      "Number of SKUs for the attribute hem_type: 158\n",
      "Start Buckets\n",
      "9 Attribute: jacket_style\n",
      "Number of SKUs for the attribute jacket_style: 371\n",
      "Start Buckets\n",
      "10 Attribute: lapel_style\n",
      "Number of SKUs for the attribute lapel_style: 316\n",
      "Start Buckets\n",
      "11 Attribute: lining\n",
      "Number of SKUs for the attribute lining: 306\n",
      "Start Buckets\n",
      "12 Attribute: material\n",
      "Number of SKUs for the attribute material: 1973\n",
      "Start Buckets\n",
      "13 Attribute: metal_tone\n",
      "Number of SKUs for the attribute metal_tone: 77\n",
      "Start Buckets\n",
      "14 Attribute: neckline\n",
      "Number of SKUs for the attribute neckline: 85\n",
      "Start Buckets\n",
      "15 Attribute: occasion\n",
      "Number of SKUs for the attribute occasion: 1581\n",
      "Start Buckets\n",
      "16 Attribute: package_quantity\n",
      "Number of SKUs for the attribute package_quantity: 489\n",
      "Start Buckets\n",
      "17 Attribute: pant_style\n",
      "Number of SKUs for the attribute pant_style: 188\n",
      "Start Buckets\n",
      "18 Attribute: pattern\n",
      "Number of SKUs for the attribute pattern: 1802\n",
      "Start Buckets\n",
      "19 Attribute: pocket_style\n",
      "Number of SKUs for the attribute pocket_style: 994\n",
      "Start Buckets\n",
      "20 Attribute: product_type\n",
      "Number of SKUs for the attribute product_type: 2251\n",
      "Start Buckets\n",
      "21 Attribute: quantity\n",
      "Number of SKUs for the attribute quantity: 1\n",
      "Start Buckets\n",
      "22 Attribute: scent\n",
      "Number of SKUs for the attribute scent: 22\n",
      "Start Buckets\n",
      "23 Attribute: sleeve_length\n",
      "Number of SKUs for the attribute sleeve_length: 749\n",
      "Start Buckets\n",
      "24 Attribute: sock_style\n",
      "Number of SKUs for the attribute sock_style: 98\n",
      "Start Buckets\n",
      "25 Attribute: sole_material\n",
      "Number of SKUs for the attribute sole_material: 75\n",
      "Start Buckets\n",
      "26 Attribute: tie_width\n",
      "Number of SKUs for the attribute tie_width: 288\n",
      "Start Buckets\n",
      "27 Attribute: toe_shape\n",
      "Number of SKUs for the attribute toe_shape: 86\n",
      "Start Buckets\n",
      "28 Attribute: toe_style\n",
      "Number of SKUs for the attribute toe_style: 82\n",
      "Start Buckets\n",
      "29 Attribute: underwear_style\n",
      "Number of SKUs for the attribute underwear_style: 7\n",
      "Start Buckets\n",
      "30 Attribute: vent_style\n",
      "Number of SKUs for the attribute vent_style: 318\n",
      "Start Buckets\n",
      "31 Attribute: volume\n",
      "Number of SKUs for the attribute volume: 16\n",
      "Start Buckets\n",
      "32 Attribute: wash\n",
      "Number of SKUs for the attribute wash: 1\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted(cb['attribute'].explode().value_counts().reset_index()['index'].to_list())))\n",
    "lst_attribute=sorted(cb['attribute'].explode().value_counts().reset_index()['index'].to_list())\n",
    "print('Number of attributes: '+str(len(lst_attribute)))\n",
    "print('')\n",
    "\n",
    "# lst_attribute=['color']\n",
    "for i in range(len(lst_attribute)):\n",
    "    attri=lst_attribute[i]\n",
    "    dateszs='2001-08-11'\n",
    "    attribut=attri\n",
    "    dateszs='2001-11-01'\n",
    "    curation_col = f'Q:{attribut}'\n",
    "    params = {'customer_id': customer_id,\n",
    "              'customer_name':customer_name,\n",
    "             'dateszs':dateszs,\n",
    "             'attribut':attribut}\n",
    "\n",
    "    print('Start Buckets')\n",
    "    print(str(i)+' Attribute: '+str(attri))\n",
    "    bucket_value = query_from_file(file_name='query/Bucket_Value_Strategy.sql', params=params)\n",
    "\n",
    "    brand=cb[(cb['value'].astype(str)!='[\"n/a\"]')&(cb['value'].astype(str)!='n/a')&(cb['attribute'].astype(str)==attri)]\n",
    "#     print('Number of SKUs for the attribute '+str(attri)+': '+str(len(brand)))\n",
    "    list_of_all_buckets=sorted(list(set(bucket_value['buckets'].to_list())))\n",
    "\n",
    "    brand=cb[(cb['value'].astype(str)!='[\"n/a\"]')&(cb['value'].astype(str)!='n/a')&(cb['attribute'].astype(str)==attri)]\n",
    "    print('Number of SKUs for the attribute '+str(attri)+': '+str(len(brand)))\n",
    "    list_of_all_buckets=sorted(list(set(bucket_value['buckets'].to_list())))\n",
    "\n",
    "    filtered_dfs = {}\n",
    "    b_lst=[]\n",
    "\n",
    "    for i in range(len(list_of_all_buckets)):\n",
    "\n",
    "        # values that are supposed to be in the buckets\n",
    "        action_bucket_values=bucket_value[bucket_value['buckets'].astype(str)==list_of_all_buckets[i]]['values'].to_list()\n",
    "\n",
    "        # values that are actually in the curation\n",
    "        action=brand[brand['buckets'].astype(str)==list_of_all_buckets[i]]['value'].to_list()\n",
    "\n",
    "        unique_values=get_unique_list(action)\n",
    "\n",
    "        target=[x for x in unique_values if x not in action_bucket_values]\n",
    "        if len(target) > 0:\n",
    "            target = target[0]\n",
    "            pat=f'''(?i)({target})|()'''\n",
    "            bucket_filtered_df=brand[brand['buckets'].astype(str)==list_of_all_buckets[i]]\n",
    "            bucket_filtered_df['match']=bucket_filtered_df['value'].apply(lambda x: re_extract(pat,str(x)))\n",
    "            fil=bucket_filtered_df[bucket_filtered_df['match'].astype(str)!='[]']\n",
    "            filtered_dfs[list_of_all_buckets[i]] = fil\n",
    "            b_lst.append(list_of_all_buckets[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    b_lst[0:10]\n",
    "    valuez = {}\n",
    "    external_id_list=[]\n",
    "    for i in range(len(b_lst)):\n",
    "        valuez[b_lst[i]]=filtered_dfs[b_lst[i]][['external_id','value']]\n",
    "        external_id_list.append(filtered_dfs[b_lst[i]]['external_id'].tolist())\n",
    "    import itertools\n",
    "    flattened_list = list(itertools.chain.from_iterable(external_id_list))\n",
    "    if len(flattened_list)>0:\n",
    "        print('Buckets Effected: '+str(len(b_lst)))\n",
    "        df_name = f'match_{attri}'\n",
    "        data = {'external_id': flattened_list, f'Q:{attri}': '[]'}\n",
    "        df_dict = {df_name: pd.DataFrame(data)}\n",
    "        new_df = df_dict[df_name]\n",
    "        print('Number of SKUs to be wiped: '+str(len(new_df)))\n",
    "        print('')\n",
    "        print('')\n",
    "\n",
    "        def get_df_name(df):\n",
    "            name =[x for x in globals() if globals()[x] is df][0]\n",
    "            return name\n",
    "        def looks_good(customer, matches,attri): \n",
    "            drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/Ready For Upload' \n",
    "            matches.to_csv(f'{drive_path}/{client} --{get_df_name(matches)}match_{attri}-{today}.csv',index=False) \n",
    "        looks_good(client, new_df,attri) \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Freetext Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # JOB does not have any freetext attributes\n",
    "\n",
    "# patterns = {\n",
    "#     'appliance_cubic_feet': r'(cu ft)|(n\\/a)|()'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_empty_matches(dataframe, attribute, pattern):\n",
    "    matches = dataframe[dataframe['attribute'].astype(str) == attribute]['value'].apply(lambda x: re.findall(pattern, str(x)))\n",
    "    empty_matches = matches.apply(lambda x: len(x) == 0)\n",
    "    return dataframe[dataframe['attribute'].astype(str) == attribute][empty_matches]\n",
    "\n",
    "def count_empty_matches(dataframe, attribute, pattern):\n",
    "    print(f'Number of SKUs for {attribute}: '+str(len(dataframe[dataframe['attribute'].astype(str) == attribute])))\n",
    "    matches = dataframe[dataframe['attribute'].astype(str) == attribute]['value'].apply(lambda x: re.findall(pattern, str(x)))\n",
    "    return sum(len(match) == 0 for match in matches)\n",
    "\n",
    "\n",
    "filtered_dataframes = {}\n",
    "for attribute, pattern in patterns.items():\n",
    "    filtered_dataframes[attribute] = filter_empty_matches(cb, attribute, pattern)\n",
    "    \n",
    "counts = {}\n",
    "for attribute, pattern in patterns.items():\n",
    "    counts[attribute] = count_empty_matches(cb, attribute, pattern)\n",
    "#     print(f'Rows that need to be wiped for {attribute}: '+f': {counts[attribute]}')\n",
    "   \n",
    "\n",
    "    try:\n",
    "        flattened_list = filtered_dataframes[attribute]['external_id'].to_list()\n",
    "        if len(flattened_list) > 0:\n",
    "            df_name = f'match_{attribute}'\n",
    "            data = {'external_id': flattened_list, f'Q:{attribute}': '[]'}\n",
    "            df_dict = {df_name: pd.DataFrame(data)}\n",
    "            new_df = df_dict[df_name]\n",
    "            print('Number of SKUs to be wiped: ' + str(len(new_df)))\n",
    "\n",
    "            def get_df_name(df):\n",
    "                name = [x for x in globals() if globals()[x] is df][0]\n",
    "                return name\n",
    "\n",
    "            def looks_good(customer, matches, attribute):\n",
    "                today = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
    "                drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/Ready For Upload'\n",
    "                matches.to_csv(f'{drive_path}/{client} --{get_df_name(matches)}match_{attribute}-{today}.csv', index=False)\n",
    "\n",
    "            looks_good(client, new_df, attribute)\n",
    "        else:\n",
    "            print('No SKUs to be wiped for ' + attribute)\n",
    "    except KeyError:\n",
    "        print('Error: DataFrame with name ' + df_name + ' does not exist')\n",
    "    print('')\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncurated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "formatted_attribute = 'sustainability'\n",
    "buckets = \"\"\"Office Chairs\"\"\"\n",
    "\n",
    "attribute = formatted_attribute.lower().replace(' ','_').replace('-','_')\n",
    "value='%n/a%'\n",
    "params = {'customer_id': customer_id ,\n",
    "          'attribute': attribute,\n",
    "          'buckets': str(buckets.split('\\t'))[1:-1],\n",
    "          'value':value,\n",
    "          'customer_name':customer_name\n",
    "         }\n",
    "curation_col = f'Q:{attribute}'\n",
    "\n",
    "\n",
    "df = query_from_file(file_name='./query/custom_fields.sql', params=params)\n",
    "print(len(df))\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# send to the folder for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "\n",
    "def looks_good(customer, attribute,matches,today): \n",
    "    drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/_Ready For Upload' \n",
    "    matches.to_csv(f'{drive_path}/{client} - {attribute}-{get_df_name(matches)}-{today}-matches.csv',index=False) \n",
    "    \n",
    "looks_good(client, attribute,matchesna,today)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "\n",
    "def looks_good(customer, attribute,matches,today): \n",
    "    drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/_Ready For Upload' \n",
    "    matches.to_csv(f'{drive_path}/{client} - {attribute}-{get_df_name(matches)}-{today}-matches.csv',index=False) \n",
    "    \n",
    "looks_good(client, attribute,matchesdiam,today)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
