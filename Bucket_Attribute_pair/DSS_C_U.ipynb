{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "from ftfy import fix_text\n",
    "# from util import UnitConversion, mapping_list_values, perl_to_posix\n",
    "from groupby_toolz import enrich_db, gcloud\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 500\n",
    "from flashtext import KeywordProcessor\n",
    "from groupby_toolz import enrich_db, gcloud\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import ast\n",
    "import warnings\n",
    "import regex as re\n",
    "warnings.filterwarnings('ignore')\n",
    "from decimal import *\n",
    "TWOPLACES = Decimal(10) ** -2\n",
    "from natsort import natsorted\n",
    "import ast\n",
    "import time\n",
    "today = time.strftime(\"%Y_%m_%d\")\n",
    "from enrich_dimensions.rounds import rounds, rounding_inch_feet,rounding_lbs,rounding_w,rounding_oz, rounding_lb,rounding_gal, re_extract, curate, round_string_float, clean_data,reg_ex,rounding_period_after_unit \n",
    "from enrich_dimensions.params import parameters, query_from_file\n",
    "from enrich_dimensions.query_file import query_from_file \n",
    "from enrich_dimensions.custom import custom_field \n",
    "\n",
    "def get_unique_list(material_list):\n",
    "    unique_list = []\n",
    "    for item in material_list:\n",
    "        if isinstance(item, str) and \"[\" in item and \"]\" in item:\n",
    "            item_values = ast.literal_eval(item)\n",
    "            unique_list.extend(item_values)\n",
    "        else:\n",
    "            unique_list.append(item)\n",
    "    return sorted(list(set(unique_list)))\n",
    "\n",
    "customer_id = '26'\n",
    "customer_name='%discountschoolsupply%'\n",
    "client='DSS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Query of Curated Data\n",
      "32283\n"
     ]
    }
   ],
   "source": [
    "# attribut='count'\n",
    "dateszs='2001-08-11'\n",
    "# # attribut='features'\n",
    "# dateszs='2001-11-01'\n",
    "# curation_col = f'Q:{attribut}'\n",
    "params = {'customer_id': customer_id,\n",
    "          'customer_name':customer_name,\n",
    "         'dateszs':dateszs}\n",
    "\n",
    "print('Start Query of Curated Data')\n",
    "cb = query_from_file(file_name='query/curated_all_attributes_date_family.sql', params=params)\n",
    "print(len(cb))\n",
    "\n",
    "fill_dict = cb.groupby('external_id')['buckets'].last().to_dict()\n",
    "cb['buckets'] = cb['buckets'].fillna(cb['external_id'].map(fill_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check attribute bucket pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "Number of attributes: 42\n",
      "\n",
      "Start Buckets\n",
      "0 Attribute: capacity_gal\n",
      "Number of SKUs for the attribute capacity_gal: 2\n",
      "Start Buckets\n",
      "1 Attribute: capacity_l\n",
      "Number of SKUs for the attribute capacity_l: 2\n",
      "Start Buckets\n",
      "2 Attribute: capacity_oz\n",
      "Number of SKUs for the attribute capacity_oz: 1\n",
      "Start Buckets\n",
      "3 Attribute: capacity_qt\n",
      "Number of SKUs for the attribute capacity_qt: 0\n",
      "Start Buckets\n",
      "4 Attribute: color\n",
      "Number of SKUs for the attribute color: 2517\n",
      "Start Buckets\n",
      "5 Attribute: count\n",
      "Number of SKUs for the attribute count: 5780\n",
      "Start Buckets\n",
      "6 Attribute: count_per_package\n",
      "Number of SKUs for the attribute count_per_package: 360\n",
      "Start Buckets\n",
      "7 Attribute: count_range\n",
      "Number of SKUs for the attribute count_range: 5739\n",
      "Start Buckets\n",
      "8 Attribute: culture_ethnicity\n",
      "Number of SKUs for the attribute culture_ethnicity: 20\n",
      "Start Buckets\n",
      "9 Attribute: depth\n",
      "Number of SKUs for the attribute depth: 706\n",
      "Start Buckets\n",
      "10 Attribute: diameter\n",
      "Number of SKUs for the attribute diameter: 28\n",
      "Start Buckets\n",
      "11 Attribute: features\n",
      "Number of SKUs for the attribute features: 903\n",
      "Start Buckets\n",
      "12 Attribute: finish\n",
      "Number of SKUs for the attribute finish: 21\n",
      "Start Buckets\n",
      "13 Attribute: gender\n",
      "Number of SKUs for the attribute gender: 32\n",
      "Start Buckets\n",
      "14 Attribute: glue_type\n",
      "Number of SKUs for the attribute glue_type: 13\n",
      "Start Buckets\n",
      "15 Attribute: height\n",
      "Number of SKUs for the attribute height: 1477\n",
      "Start Buckets\n",
      "16 Attribute: instrument_type\n",
      "Number of SKUs for the attribute instrument_type: 8\n",
      "Start Buckets\n",
      "17 Attribute: length\n",
      "Number of SKUs for the attribute length: 903\n",
      "Start Buckets\n",
      "18 Attribute: mat_thickness\n",
      "Number of SKUs for the attribute mat_thickness: 0\n",
      "Start Buckets\n",
      "19 Attribute: material\n",
      "Number of SKUs for the attribute material: 1188\n",
      "Start Buckets\n",
      "20 Attribute: package_quantity\n",
      "Number of SKUs for the attribute package_quantity: 560\n",
      "Buckets Effected: 1\n",
      "Number of SKUs to be wiped: 1\n",
      "\n",
      "\n",
      "Start Buckets\n",
      "21 Attribute: paper_type\n",
      "Number of SKUs for the attribute paper_type: 68\n",
      "Start Buckets\n",
      "22 Attribute: power_source\n",
      "Number of SKUs for the attribute power_source: 10\n",
      "Start Buckets\n",
      "23 Attribute: product_type\n",
      "Number of SKUs for the attribute product_type: 1848\n",
      "Buckets Effected: 1\n",
      "Number of SKUs to be wiped: 1\n",
      "\n",
      "\n",
      "Start Buckets\n",
      "24 Attribute: project\n",
      "Number of SKUs for the attribute project: 1\n",
      "Start Buckets\n",
      "25 Attribute: purpose\n",
      "Number of SKUs for the attribute purpose: 124\n",
      "Start Buckets\n",
      "26 Attribute: roll_length\n",
      "Number of SKUs for the attribute roll_length: 0\n",
      "Start Buckets\n",
      "27 Attribute: roll_width\n",
      "Number of SKUs for the attribute roll_width: 0\n",
      "Start Buckets\n",
      "28 Attribute: science_topic\n",
      "Number of SKUs for the attribute science_topic: 15\n",
      "Start Buckets\n",
      "29 Attribute: seat_material\n",
      "Number of SKUs for the attribute seat_material: 63\n",
      "Start Buckets\n",
      "30 Attribute: size\n",
      "Number of SKUs for the attribute size: 22\n",
      "Start Buckets\n",
      "31 Attribute: specialty_finish\n",
      "Number of SKUs for the attribute specialty_finish: 3\n",
      "Start Buckets\n",
      "32 Attribute: storage_options\n",
      "Number of SKUs for the attribute storage_options: 118\n",
      "Start Buckets\n",
      "33 Attribute: subject\n",
      "Number of SKUs for the attribute subject: 1210\n",
      "Start Buckets\n",
      "34 Attribute: surface_application\n",
      "Number of SKUs for the attribute surface_application: 2\n",
      "Start Buckets\n",
      "35 Attribute: tabletop_shape\n",
      "Number of SKUs for the attribute tabletop_shape: 112\n",
      "Start Buckets\n",
      "36 Attribute: theme\n",
      "Number of SKUs for the attribute theme: 1\n",
      "Start Buckets\n",
      "37 Attribute: tip_shape\n",
      "Number of SKUs for the attribute tip_shape: 3\n",
      "Start Buckets\n",
      "38 Attribute: volume_lb\n",
      "Number of SKUs for the attribute volume_lb: 10\n",
      "Start Buckets\n",
      "39 Attribute: volume_oz\n",
      "Number of SKUs for the attribute volume_oz: 467\n",
      "Start Buckets\n",
      "40 Attribute: weight\n",
      "Number of SKUs for the attribute weight: 276\n",
      "Start Buckets\n",
      "41 Attribute: width\n",
      "Number of SKUs for the attribute width: 1807\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted(cb['attribute'].explode().value_counts().reset_index()['index'].to_list())))\n",
    "lst_attribute=sorted(cb['attribute'].explode().value_counts().reset_index()['index'].to_list())\n",
    "print('Number of attributes: '+str(len(lst_attribute)))\n",
    "print('')\n",
    "\n",
    "# lst_attribute=['color']\n",
    "for i in range(len(lst_attribute)):\n",
    "    attri=lst_attribute[i]\n",
    "    dateszs='2001-08-11'\n",
    "    attribut=attri\n",
    "    dateszs='2001-11-01'\n",
    "    curation_col = f'Q:{attribut}'\n",
    "    params = {'customer_id': customer_id,\n",
    "              'customer_name':customer_name,\n",
    "             'dateszs':dateszs,\n",
    "             'attribut':attribut}\n",
    "\n",
    "    print('Start Buckets')\n",
    "    print(str(i)+' Attribute: '+str(attri))\n",
    "    bucket_value = query_from_file(file_name='query/Bucket_Value_Strategy.sql', params=params)\n",
    "\n",
    "    brand=cb[(cb['value'].astype(str)!='[\"n/a\"]')&(cb['value'].astype(str)!='n/a')&(cb['attribute'].astype(str)==attri)]\n",
    "    print('Number of SKUs for the attribute '+str(attri)+': '+str(len(brand)))\n",
    "    list_of_all_buckets=sorted(list(set(bucket_value['buckets'].to_list())))\n",
    "\n",
    "    brand=cb[(cb['value'].astype(str)!='[\"n/a\"]')&(cb['value'].astype(str)!='n/a')&(cb['attribute'].astype(str)==attri)]\n",
    "#     print('Number of SKUs for the attribute '+str(attri)+': '+str(len(brand)))\n",
    "    list_of_all_buckets=sorted(list(set(bucket_value['buckets'].to_list())))\n",
    "\n",
    "    filtered_dfs = {}\n",
    "    b_lst=[]\n",
    "\n",
    "    for i in range(len(list_of_all_buckets)):\n",
    "\n",
    "        # values that are supposed to be in the buckets\n",
    "        action_bucket_values=bucket_value[bucket_value['buckets'].astype(str)==list_of_all_buckets[i]]['values'].to_list()\n",
    "\n",
    "        # values that are actually in the curation\n",
    "        action=brand[brand['buckets'].astype(str)==list_of_all_buckets[i]]['value'].to_list()\n",
    "\n",
    "        unique_values=get_unique_list(action)\n",
    "\n",
    "        target=[x for x in unique_values if x not in action_bucket_values]\n",
    "        if len(target) > 0:\n",
    "            target = target[0]\n",
    "            pat=f'''(?i)({target})|()'''\n",
    "            bucket_filtered_df=brand[brand['buckets'].astype(str)==list_of_all_buckets[i]]\n",
    "            bucket_filtered_df['match']=bucket_filtered_df['value'].apply(lambda x: re_extract(pat,str(x)))\n",
    "            fil=bucket_filtered_df[bucket_filtered_df['match'].astype(str)!='[]']\n",
    "            filtered_dfs[list_of_all_buckets[i]] = fil\n",
    "            b_lst.append(list_of_all_buckets[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    b_lst[0:10]\n",
    "    valuez = {}\n",
    "    external_id_list=[]\n",
    "    for i in range(len(b_lst)):\n",
    "        valuez[b_lst[i]]=filtered_dfs[b_lst[i]][['external_id','value']]\n",
    "        external_id_list.append(filtered_dfs[b_lst[i]]['external_id'].tolist())\n",
    "    import itertools\n",
    "    flattened_list = list(itertools.chain.from_iterable(external_id_list))\n",
    "    if len(flattened_list)>0:\n",
    "        print('Buckets Effected: '+str(len(b_lst)))\n",
    "        df_name = f'match_{attri}'\n",
    "        data = {'external_id': flattened_list, f'Q:{attri}': '[]'}\n",
    "        df_dict = {df_name: pd.DataFrame(data)}\n",
    "        new_df = df_dict[df_name]\n",
    "        print('Number of SKUs to be wiped: '+str(len(new_df)))\n",
    "        print('')\n",
    "        print('')\n",
    "\n",
    "        def get_df_name(df):\n",
    "            name =[x for x in globals() if globals()[x] is df][0]\n",
    "            return name\n",
    "        def looks_good(customer, matches,attri): \n",
    "            drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/Ready For Upload' \n",
    "            matches.to_csv(f'{drive_path}/{client} --{get_df_name(matches)}match_{attri}-{today}.csv',index=False) \n",
    "        looks_good(client, new_df,attri) \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Freetext Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    'width': r'(\\d in)|(\\d ft)|(n\\/a)|()',\n",
    "    'volume_oz': r'(\\d oz)|(n\\/a)|()',\n",
    "    'volume_lb': r'(\\d lb)|(n\\/a)|()',\n",
    "#     'volume_gal':r'(\\d gal)|(n\\/a)|()',\n",
    "    'length':r'(\\d in)|(\\d ft)|(n\\/a)|()',\n",
    "    'diameter':r'(\\d in)|(\\d ft)|(n\\/a)|()',\n",
    "    'depth':r'(\\d in)|(\\d ft)|(n\\/a)|()',\n",
    "    'capacity_qt':r'(\\d+ qt)|(n\\/a)|()',\n",
    "    'capacity_oz':r'(\\d+ oz)|(n\\/a)|()',\n",
    "    'capacity_l':r'(\\d+ L)|(n\\/a)|()',\n",
    "    'capacity_gal':r'(\\d+ gal)|(n\\/a)|()',\n",
    "#     'count':r'(\\d+\\,)|(n\\/a)|()'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cb[cb['attribute'].astype(str)=='count']['value'].explode().value_counts()[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SKUs for width: 2587\n",
      "Number of SKUs to be wiped: 10\n",
      "\n",
      "\n",
      "Number of SKUs for volume_oz: 1636\n",
      "Number of SKUs to be wiped: 1\n",
      "\n",
      "\n",
      "Number of SKUs for volume_lb: 26\n",
      "No SKUs to be wiped for volume_lb\n",
      "Number of SKUs for volume_gal: 0\n",
      "Error: DataFrame with name match_volume_oz does not exist\n",
      "Number of SKUs for length: 1646\n",
      "Number of SKUs to be wiped: 2\n",
      "\n",
      "\n",
      "Number of SKUs for diameter: 112\n",
      "No SKUs to be wiped for diameter\n",
      "Number of SKUs for depth: 1303\n",
      "Number of SKUs to be wiped: 4\n",
      "\n",
      "\n",
      "Number of SKUs for capacity_qt: 2\n",
      "No SKUs to be wiped for capacity_qt\n",
      "Number of SKUs for capacity_oz: 3\n",
      "No SKUs to be wiped for capacity_oz\n",
      "Number of SKUs for capacity_l: 2\n",
      "No SKUs to be wiped for capacity_l\n",
      "Number of SKUs for capacity_gal: 2\n",
      "No SKUs to be wiped for capacity_gal\n",
      "Number of SKUs for count: 5845\n",
      "Number of SKUs to be wiped: 5777\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_empty_matches(dataframe, attribute, pattern):\n",
    "    matches = dataframe[dataframe['attribute'].astype(str) == attribute]['value'].apply(lambda x: re_extract(pattern,str(x)))\n",
    "    empty_matches = matches.apply(lambda x: len(x) == 0)\n",
    "    return dataframe[dataframe['attribute'].astype(str) == attribute][empty_matches]\n",
    "\n",
    "def count_empty_matches(dataframe, attribute, pattern):\n",
    "    print(f'Number of SKUs for {attribute}: '+str(len(dataframe[dataframe['attribute'].astype(str) == attribute])))\n",
    "    matches = dataframe[dataframe['attribute'].astype(str) == attribute]['value'].apply(lambda x: re_extract(pattern,str(x)))\n",
    "    return sum(len(match) == 0 for match in matches)\n",
    "\n",
    "\n",
    "filtered_dataframes = {}\n",
    "for attribute, pattern in patterns.items():\n",
    "    filtered_dataframes[attribute] = filter_empty_matches(cb, attribute, pattern)\n",
    "    \n",
    "counts = {}\n",
    "for attribute, pattern in patterns.items():\n",
    "    counts[attribute] = count_empty_matches(cb, attribute, pattern)\n",
    "#     print(f'Rows that need to be wiped for {attribute}: '+f': {counts[attribute]}')\n",
    "   \n",
    "\n",
    "    try:\n",
    "        flattened_list = filtered_dataframes[attribute]['external_id'].to_list()\n",
    "        if len(flattened_list) > 0:\n",
    "            df_name = f'match_{attribute}'\n",
    "            data = {'external_id': flattened_list, f'Q:{attribute}': '[]'}\n",
    "            df_dict = {df_name: pd.DataFrame(data)}\n",
    "            new_df = df_dict[df_name]\n",
    "            print('Number of SKUs to be wiped: ' + str(len(new_df)))\n",
    "            print('')\n",
    "            print('')\n",
    "\n",
    "            def get_df_name(df):\n",
    "                name = [x for x in globals() if globals()[x] is df][0]\n",
    "                return name\n",
    "\n",
    "            def looks_good(customer, matches, attribute):\n",
    "                today = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
    "                drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/Ready For Upload'\n",
    "                matches.to_csv(f'{drive_path}/{client} --{get_df_name(matches)}match_{attribute}-{today}.csv', index=False)\n",
    "            looks_good(client, new_df, attribute)\n",
    "            \n",
    "        else:\n",
    "            print('No SKUs to be wiped for ' + attribute)\n",
    "    except KeyError:\n",
    "        print('Error: DataFrame with name ' + df_name + ' does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5799\n",
      "5845\n"
     ]
    }
   ],
   "source": [
    "count_range=cb[cb['attribute'].astype(str)=='count_range']\n",
    "count=cb[cb['attribute'].astype(str)=='count']\n",
    "\n",
    "print(len(count_range))\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_range['Q:count_range']='[]'\n",
    "match_count_range=count_range[['external_id','Q:count_range']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateszs='2002-08-11'\n",
    "attribut='count'\n",
    "curation_col = f'Q:{attribut}'\n",
    "params = {'customer_id': customer_id,\n",
    "          'customer_name':customer_name,\n",
    "         'dateszs':dateszs,\n",
    "         'attribute':attribut}\n",
    "\n",
    "print('start')\n",
    "cb = query_from_file(file_name='./query/curated_all_attributes_date_one_attribute.sql', params=params)\n",
    "print(len(cb))\n",
    "# dfz=dfs[dfs['resolution'].astype(str)!='rules']\n",
    "# print(len(dfz))\n",
    "# dfh=dfz[dfz['custom_fields'].astype(str)!='None']\n",
    "# # dfz=dfz[dfz['buckets'].astype(str)=='Conduit Fittings, Replacement Parts, & Accessories']\n",
    "# print(len(dfh))\n",
    "# dfh=dfs[dfs['custom_fields'].astype(str)!=['None']]\n",
    "# custom_field_df=pd.json_normalize(dfs['custom_fields'])\n",
    "# fields=['Set_Size']\n",
    "# df = pd.concat([dfs.drop('custom_fields', axis=1), custom_field_df[fields]], axis = 1)\n",
    "# # print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat='''(\\d+(?!\\d)(?!\\-))|(individual)|(Indivdual)|(Individual in)|(Inividual)|()'''\n",
    "count['match']=count['value'].apply(lambda x: re_extract(pat,str(x)))\n",
    "c=count[count['match'].astype(str)!='[]']\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['Q:count']='[]'\n",
    "match_count_wipe=c[['external_id','Q:count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_df_name(df):\n",
    "#     name =[x for x in globals() if globals()[x] is df][0]\n",
    "#     return name\n",
    "# def looks_good(customer, matches): \n",
    "#     drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/Ready For Upload' \n",
    "#     matches.to_csv(f'{drive_path}/{client} --{get_df_name(matches)}match_count_wipe-{today}.csv',index=False) \n",
    "# looks_good(client, match_count_wipe) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncurated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "2023_06_03\n"
     ]
    }
   ],
   "source": [
    "formatted_attribute='count_range'\n",
    "buckets = \"\"\"Office Chairs\"\"\"\n",
    "\n",
    "# strategy_version_input=767\n",
    "# attribute_id_input=2730\n",
    "\n",
    "attribute = formatted_attribute.lower().replace(' ','_').replace('-','_')\n",
    "value='%n/a%'\n",
    "params = {'customer_id': customer_id ,\n",
    "          'attribute': attribute,\n",
    "          'buckets': str(buckets.split('\\t'))[1:-1],\n",
    "          'value':value,\n",
    "          'customer_name':customer_name\n",
    "         }\n",
    "curation_col = f'Q:{attribute}'\n",
    "\n",
    "\n",
    "df = query_from_file(file_name='./query/custom_fields.sql', params=params)\n",
    "print(len(df))\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id=df['external_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=count[(count['external_id'].isin(df_id))&(count['value'].astype(str)!='Individual')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50    1\n",
       "Name: value, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x))\n",
    "x['value'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['Q:count']='[]'\n",
    "match_count=x[['external_id','Q:count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat='''(?i)(.{0,30}(?:single|individual(?!ly wrap)(?!ity)).{0,30})|()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m_long']=df['long_desc'].apply(lambda x: re_extract(pat,str(x)))\n",
    "df['m_name']=df['product_name'].apply(lambda x: re_extract(pat,str(x)))\n",
    "df['m_custom']=df['custom_fields'].apply(lambda x: re_extract(pat,str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss=df[(df['m_name'].astype(str)!='[]')|(df['m_long'].astype(str)!='[]')|(df['m_custom'].astype(str)!='[]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(dss))\n",
    "# dss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "dss_na=df[(df['m_name'].astype(str)=='[]')&(df['m_long'].astype(str)=='[]')&(df['m_custom'].astype(str)=='[]')]\n",
    "print(len(dss_na))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "def looks_good(customer, matches): \n",
    "    drive_path = f'G:/Shared drives/GroupBy Public/Customer Success/.Enrich/Platform Upload Trail/{customer}/Ready For Upload' \n",
    "    matches.to_csv(f'{drive_path}/{client} --{get_df_name(matches)}match_count-{today}.csv',index=False) \n",
    "looks_good(client, match_count) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
